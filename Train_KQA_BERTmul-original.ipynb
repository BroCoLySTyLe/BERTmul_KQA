{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU count 1\n",
      "GPU for Training : cuda \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import KQA_Dataset_BG_BERTmul as kqd\n",
    "import KQA_Model_BERTmul as kqm\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# from torch.nn.parallel import DistributedDataParallel\n",
    "# from apex.parallel import DistributedDataParallel as \"DDP\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (\"GPU count {}\".format(torch.cuda.device_count()))\n",
    "print (\"GPU for Training : {} \".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Load\n"
     ]
    }
   ],
   "source": [
    "KorQuAD_Model= kqm.QuestionAnswering()\n",
    "# KorQuAD_Model = nn.DataParallel(KorQuAD_Model)\n",
    "KorQuAD_Model.to(device)\n",
    "print(\"Model Load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KorQuAD_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = kqd.load_and_cache_dataset(input_file=\"./KorQuAD_v1.0_train.json\",\n",
    "                                           max_seq_length=512,\n",
    "                                           testset_ratio=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177854978"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_trainable_parameters(KorQuAD_Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53471"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2815"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size=8\n",
    "learning_rate = 5e-5\n",
    "\n",
    "train_dataloader = DataLoader(train_data,batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "test_dataloader = DataLoader(test_data,batch_size=batch_size,shuffle=True,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1, step:100, loss:6.222562313079834\n",
      "epoch:1, step:200, loss:6.020942211151123\n",
      "epoch:1, step:300, loss:5.925327301025391\n",
      "epoch:1, step:400, loss:5.981278419494629\n",
      "epoch:1, step:500, loss:5.670592308044434\n",
      "epoch:1, step:600, loss:5.611647605895996\n",
      "epoch:1, step:700, loss:5.4933762550354\n",
      "epoch:1, step:800, loss:5.36922550201416\n",
      "epoch:1, step:900, loss:5.401523590087891\n",
      "epoch:1, step:1000, loss:5.27094841003418\n",
      "epoch:1, step:1100, loss:5.1262383460998535\n",
      "epoch:1, step:1200, loss:5.368454933166504\n",
      "epoch:1, step:1300, loss:4.974310874938965\n",
      "epoch:1, step:1400, loss:5.06218147277832\n",
      "epoch:1, step:1500, loss:4.964707374572754\n",
      "epoch:1, step:1600, loss:5.048808574676514\n",
      "epoch:1, step:1700, loss:4.812286376953125\n",
      "epoch:1, step:1800, loss:4.9892258644104\n",
      "epoch:1, step:1900, loss:4.633874893188477\n",
      "epoch:1, step:2000, loss:4.759974479675293\n",
      "epoch:1, step:2100, loss:4.480026721954346\n",
      "epoch:1, step:2200, loss:4.472637176513672\n",
      "epoch:1, step:2300, loss:4.794617652893066\n",
      "epoch:1, step:2400, loss:4.29863166809082\n",
      "epoch:1, step:2500, loss:4.393321990966797\n",
      "epoch:1, step:2600, loss:4.641508102416992\n",
      "epoch:1, step:2700, loss:4.323710918426514\n",
      "epoch:1, step:2800, loss:4.952643394470215\n",
      "epoch:1, step:2900, loss:4.1693010330200195\n",
      "epoch:1, step:3000, loss:4.513876914978027\n",
      "epoch:1, step:3100, loss:4.4212965965271\n",
      "epoch:1, step:3200, loss:4.474275588989258\n",
      "epoch:1, step:3300, loss:4.51828145980835\n",
      "epoch:1, step:3400, loss:4.087684631347656\n",
      "epoch:1, step:3500, loss:4.362989902496338\n",
      "epoch:1, step:3600, loss:3.922517776489258\n",
      "epoch:1, step:3700, loss:3.9007797241210938\n",
      "epoch:1, step:3800, loss:4.100278377532959\n",
      "epoch:1, step:3900, loss:4.4576416015625\n",
      "epoch:1, step:4000, loss:4.376553535461426\n",
      "epoch:1, step:4100, loss:3.608295202255249\n",
      "epoch:1, step:4200, loss:4.017891883850098\n",
      "epoch:1, step:4300, loss:3.8808188438415527\n",
      "epoch:1, step:4400, loss:3.909975290298462\n",
      "epoch:1, step:4500, loss:4.055780410766602\n",
      "epoch:1, step:4600, loss:3.9109086990356445\n",
      "epoch:1, step:4700, loss:3.784480333328247\n",
      "epoch:1, step:4800, loss:4.602883338928223\n",
      "epoch:1, step:4900, loss:3.8953113555908203\n",
      "epoch:1, step:5000, loss:4.226284027099609\n",
      "epoch:1, step:5100, loss:3.8844027519226074\n",
      "epoch:1, step:5200, loss:3.6955084800720215\n",
      "epoch:1, step:5300, loss:4.223397254943848\n",
      "epoch:1, step:5400, loss:3.6409590244293213\n",
      "epoch:1, step:5500, loss:4.319286346435547\n",
      "epoch:1, step:5600, loss:4.100872993469238\n",
      "epoch:1, step:5700, loss:3.5759735107421875\n",
      "epoch:1, step:5800, loss:4.434751510620117\n",
      "epoch:1, step:5900, loss:4.127556800842285\n",
      "epoch:1, step:6000, loss:3.761679172515869\n",
      "epoch:1, step:6100, loss:3.8437438011169434\n",
      "epoch:1, step:6200, loss:3.566707134246826\n",
      "epoch:1, step:6300, loss:3.9073326587677\n",
      "epoch:1, step:6400, loss:4.2793145179748535\n",
      "epoch:1, step:6500, loss:4.0890116691589355\n",
      "epoch:1, step:6600, loss:3.9966368675231934\n",
      "epoch:1  , train mean loss:4.532695770263672\n",
      "epoch:1  , test mean loss:3.719718824424635\n",
      "epoch:2, step:100, loss:3.513164520263672\n",
      "epoch:2, step:200, loss:4.138806343078613\n",
      "epoch:2, step:300, loss:3.1593565940856934\n",
      "epoch:2, step:400, loss:3.239980697631836\n",
      "epoch:2, step:500, loss:3.856903076171875\n",
      "epoch:2, step:600, loss:3.796110153198242\n",
      "epoch:2, step:700, loss:3.871164321899414\n",
      "epoch:2, step:800, loss:3.342097520828247\n",
      "epoch:2, step:900, loss:3.880016326904297\n",
      "epoch:2, step:1000, loss:4.302685737609863\n",
      "epoch:2, step:1100, loss:3.2006959915161133\n",
      "epoch:2, step:1200, loss:3.264842987060547\n",
      "epoch:2, step:1300, loss:3.891993999481201\n",
      "epoch:2, step:1400, loss:3.898200511932373\n",
      "epoch:2, step:1500, loss:3.788088321685791\n",
      "epoch:2, step:1600, loss:3.4446802139282227\n",
      "epoch:2, step:1700, loss:3.369358777999878\n",
      "epoch:2, step:1800, loss:3.543539524078369\n",
      "epoch:2, step:1900, loss:3.4796247482299805\n",
      "epoch:2, step:2000, loss:3.1593775749206543\n",
      "epoch:2, step:2100, loss:3.701483726501465\n",
      "epoch:2, step:2200, loss:3.430370330810547\n",
      "epoch:2, step:2300, loss:4.080182075500488\n",
      "epoch:2, step:2400, loss:3.111966133117676\n",
      "epoch:2, step:2500, loss:4.845953941345215\n",
      "epoch:2, step:2600, loss:3.410278797149658\n",
      "epoch:2, step:2700, loss:3.962782382965088\n",
      "epoch:2, step:2800, loss:3.9333581924438477\n",
      "epoch:2, step:2900, loss:4.63673210144043\n",
      "epoch:2, step:3000, loss:3.1750926971435547\n",
      "epoch:2, step:3100, loss:2.893333911895752\n",
      "epoch:2, step:3200, loss:3.1039392948150635\n",
      "epoch:2, step:3300, loss:4.063485622406006\n",
      "epoch:2, step:3400, loss:4.1519317626953125\n",
      "epoch:2, step:3500, loss:3.8402748107910156\n",
      "epoch:2, step:3600, loss:4.370104789733887\n",
      "epoch:2, step:3700, loss:3.872565269470215\n",
      "epoch:2, step:3800, loss:4.040579319000244\n",
      "epoch:2, step:3900, loss:3.335252285003662\n",
      "epoch:2, step:4000, loss:4.099594593048096\n",
      "epoch:2, step:4100, loss:3.0825681686401367\n",
      "epoch:2, step:4200, loss:2.9921340942382812\n",
      "epoch:2, step:4300, loss:3.2919726371765137\n",
      "epoch:2, step:4400, loss:4.834086894989014\n",
      "epoch:2, step:4500, loss:3.331066608428955\n",
      "epoch:2, step:4600, loss:3.964505672454834\n",
      "epoch:2, step:4700, loss:3.797062397003174\n",
      "epoch:2, step:4800, loss:3.751020908355713\n",
      "epoch:2, step:4900, loss:3.172261953353882\n",
      "epoch:2, step:5000, loss:3.597240447998047\n",
      "epoch:2, step:5100, loss:3.4774584770202637\n",
      "epoch:2, step:5200, loss:3.3415069580078125\n",
      "epoch:2, step:5300, loss:2.792820930480957\n",
      "epoch:2, step:5400, loss:4.673871040344238\n",
      "epoch:2, step:5500, loss:3.758739948272705\n",
      "epoch:2, step:5600, loss:3.18618106842041\n",
      "epoch:2, step:5700, loss:3.424367666244507\n",
      "epoch:2, step:5800, loss:3.551744222640991\n",
      "epoch:2, step:5900, loss:3.936954975128174\n",
      "epoch:2, step:6000, loss:3.527777671813965\n",
      "epoch:2, step:6100, loss:3.503953695297241\n",
      "epoch:2, step:6200, loss:3.1087381839752197\n",
      "epoch:2, step:6300, loss:3.57151198387146\n",
      "epoch:2, step:6400, loss:3.7899391651153564\n",
      "epoch:2, step:6500, loss:4.298558235168457\n",
      "epoch:2, step:6600, loss:3.9071760177612305\n",
      "epoch:2  , train mean loss:3.70042085647583\n",
      "epoch:2  , test mean loss:3.4851604404612484\n",
      "epoch:3, step:100, loss:3.868133783340454\n",
      "epoch:3, step:200, loss:3.297964334487915\n",
      "epoch:3, step:300, loss:3.560013771057129\n",
      "epoch:3, step:400, loss:3.7224957942962646\n",
      "epoch:3, step:500, loss:2.924254894256592\n",
      "epoch:3, step:600, loss:3.666779041290283\n",
      "epoch:3, step:700, loss:4.478582859039307\n",
      "epoch:3, step:800, loss:3.3609766960144043\n",
      "epoch:3, step:900, loss:3.572636365890503\n",
      "epoch:3, step:1000, loss:3.8382349014282227\n",
      "epoch:3, step:1100, loss:3.443586826324463\n",
      "epoch:3, step:1200, loss:3.6758241653442383\n",
      "epoch:3, step:1300, loss:3.7014048099517822\n",
      "epoch:3, step:1400, loss:3.07292103767395\n",
      "epoch:3, step:1500, loss:4.377175331115723\n",
      "epoch:3, step:1600, loss:3.7832462787628174\n",
      "epoch:3, step:1700, loss:4.161031723022461\n",
      "epoch:3, step:1800, loss:3.2925446033477783\n",
      "epoch:3, step:1900, loss:3.1895041465759277\n",
      "epoch:3, step:2000, loss:3.6488852500915527\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    train_meanloss=0\n",
    "    for i,(input_ids, segment_ids, input_mask, start_positions, end_positions) in enumerate(train_dataloader):\n",
    "        input_ids = torch.tensor(input_ids,dtype=torch.long)\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = torch.tensor(input_mask,dtype=torch.long)\n",
    "        input_mask = input_mask.to(device)\n",
    "        segment_ids = torch.tensor(segment_ids,dtype=torch.long)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        start_positions = torch.tensor(start_positions,dtype=torch.long)\n",
    "        start_positions = start_positions.to(device)\n",
    "        end_positions = torch.tensor(end_positions,dtype=torch.long)\n",
    "        end_positions = end_positions.to(device)\n",
    "        #KorQuAD_Model = KorQuAD_Model.to(device)\n",
    "        #optimizer = torch.optim.SGD(mymodel.parameters(), lr=0.0001)\n",
    "        optimizer = torch.optim.SGD(KorQuAD_Model.parameters(), lr=learning_rate)\n",
    "        optimizer.zero_grad()\n",
    "        KorQuAD_Model.train()\n",
    "        loss = KorQuAD_Model(input_ids, segment_ids, input_mask, start_positions, end_positions)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_meanloss+=loss\n",
    "        if (i+1)%100==0:\n",
    "            print(\"epoch:{}, step:{}, loss:{}\".format(epoch+1,i+1,loss))\n",
    "    train_meanloss=train_meanloss/i\n",
    "    train_meanloss=train_meanloss.item() \n",
    "    print(\"epoch:{}  , train mean loss:{}\".format((epoch+1),train_meanloss))\n",
    "    test_meanloss=0\n",
    "    for j,(input_ids, segment_ids, input_mask, start_positions, end_positions) in enumerate(test_dataloader):\n",
    "        with torch.no_grad():\n",
    "            input_ids = torch.tensor(input_ids,dtype=torch.long)\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = torch.tensor(input_mask,dtype=torch.long)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = torch.tensor(segment_ids,dtype=torch.long)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            start_positions = torch.tensor(start_positions,dtype=torch.long)\n",
    "            start_positions = start_positions.to(device)\n",
    "            end_positions = torch.tensor(end_positions,dtype=torch.long)\n",
    "            \n",
    "            end_positions = end_positions.to(device)\n",
    "            KorQuAD_Model.eval()\n",
    "            test_loss = KorQuAD_Model(input_ids, segment_ids, input_mask, start_positions, end_positions)\n",
    "            test_meanloss+=test_loss.item()\n",
    "    test_meanloss=test_meanloss/j\n",
    "#     test_meanloss=test_meanloss.item()\n",
    "    print(\"epoch:{}  , test mean loss:{}\".format((epoch+1),test_meanloss))\n",
    "    torch.save({'epoch':epoch,\n",
    "                'model_state_dict':KorQuAD_Model.state_dict(),\n",
    "                'optimizer_state_dict':optimizer.state_dict(),\n",
    "                'loss':loss\n",
    "               },\"/data/KorQuad_saved_models/KQA_MulBert_lr{}_bs{}_ep{}_tl{}.ckpt\".format(learning_rate,batch_size,epoch+1,test_meanloss))   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataloader = DataLoader(test_data,batch_size=1,shuffle=True,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em=0\n",
    "for j,(input_ids, segment_ids, input_mask, start_positions, end_positions) in enumerate(result_dataloader):\n",
    "    with torch.no_grad():\n",
    "        input_ids = torch.tensor(input_ids,dtype=torch.long)\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = torch.tensor(input_mask,dtype=torch.long)\n",
    "        input_mask = input_mask.to(device)\n",
    "        segment_ids = torch.tensor(segment_ids,dtype=torch.long)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        start_positions = torch.tensor(start_positions,dtype=torch.long)\n",
    "        start_positions = start_positions.to(device)\n",
    "        end_positions = torch.tensor(end_positions,dtype=torch.long)\n",
    "        end_positions = end_positions.to(device)\n",
    "        KorQuAD_Model.eval()\n",
    "        logit = KorQuAD_Model(input_ids, segment_ids, input_mask)\n",
    "        \n",
    "#         print(torch.argmax(logit[0],dim=-1))\n",
    "        \n",
    "#         print(start_positions)\n",
    "        \n",
    "#         print(torch.argmax(logit[0],dim=-1)==start_positions)\n",
    "# #         print((torch.argmax(logit[0],dim=-1)==start_positions))\n",
    "#         print(torch.argmax(logit[1],dim=-1)==end_positions)\n",
    "\n",
    "#         print((torch.argmax(logit[0],dim=-1)==start_positions) & (torch.argmax(logit[1],dim=-1)==end_positions) )    \n",
    "    \n",
    "#         print(torch.sum((torch.argmax(logit[0],dim=-1)==start_positions) & (torch.argmax(logit[1],dim=-1)==end_positions)).item())\n",
    "        \n",
    "# #         print((torch.argmax(logit,dim=-1)))\n",
    "#         em+=torch.sum((torch.argmax(logit[0],dim=-1)==start_positions) & (torch.argmax(logit[1],dim=-1)==end_positions)).item()\n",
    "        \n",
    "#         if j>-1:\n",
    "#             break\n",
    "        \n",
    "        if((torch.argmax(logit[0])==start_positions) and (torch.argmax(logit[1])==end_positions)):\n",
    "            em+=1\n",
    "#             print(em/j)\n",
    "#         if(torch.argmax(logit[0])==start_positions):\n",
    "#             em+=1\n",
    "#             print(em/j+1)\n",
    "#         if(torch.argmax(logit[1])==end_positions):\n",
    "#             em+=1\n",
    "#             print(em/j+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em/len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"/data/KorQuad_saved_models/model_ep1.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=5\n",
    "sample_input_ids = torch.tensor(test_data[idx:idx+1][0],dtype=torch.long)\n",
    "sample_input_ids = sample_input_ids.to(device)\n",
    "sample_input_mask = torch.tensor(test_data[idx:idx+1][1],dtype=torch.long)\n",
    "sample_input_mask = sample_input_mask.to(device)\n",
    "sample_segment_ids = torch.tensor(test_data[idx:idx+1][2],dtype=torch.long)\n",
    "sample_segment_ids = sample_segment_ids.to(device)\n",
    "sample_start = torch.tensor(test_data[idx:idx+1][3],dtype=torch.long)\n",
    "sample_start = sample_start.to(device)\n",
    "sample_end = torch.tensor(test_data[idx:idx+1][4],dtype=torch.long)\n",
    "sample_end = sample_end.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(119, device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_positions=torch.argmax(KorQuAD_Model(sample_input_ids,sample_input_mask,sample_segment_ids)[0])\n",
    "start_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(121, device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_positions=torch.argmax(KorQuAD_Model(sample_input_ids,sample_input_mask,sample_segment_ids)[1])\n",
    "end_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 1978년에 벌어진 노폐물을 감방에 그대로 배출하던 투쟁을 이르는 말은?[SEP] 아일랜드 단식투쟁(아일랜드어: Stailc ocrais 스탈크 오크라스, 영어: Irish hunger strike)은 북아일랜드 분쟁이 진행 중이던 1981년, 북아일랜드의 감옥에 수감되어 있던 아일랜드 공화주의자들이 자신들을 일반 죄수가 아닌 양심수로서 대우해 달라고 요구하며 벌인 단식투쟁이다. 1976년에 모포투쟁으로 시작된 5년간의 양심수 대우 요구 저항 행동의 절정을 장식한 사건이다. 1978년에는 수감자가 감방 밖으로 나가기를 거부하며 배설물을 비롯한 노폐물을 감방에 그대로 배출하는 이른바 불결투쟁이 벌어졌다. 그런데도 영국 정부가 자신들의 요구를 들어줄 기미가 보이지 않자 마침내 1980년, 열 명의 수감자가 제1차 단식투쟁에 돌입해 53일간 단식했다.[SEP] ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ '"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(sample_input_ids.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gound Truth : 불결투쟁\n",
      "Model Prediction : 1976년에\n"
     ]
    }
   ],
   "source": [
    "print (\"Gound Truth : {}\\nModel Prediction : {}\".format(\n",
    "    tokenizer.decode(sample_input_ids.squeeze().tolist()[sample_start:sample_end+1]),\n",
    "    tokenizer.decode(sample_input_ids.squeeze().tolist()[start_positions:end_positions+1]))\n",
    "      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'checkpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-e889452e309c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mKorQuAD_Model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'checkpoint' is not defined"
     ]
    }
   ],
   "source": [
    "KorQuAD_Model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0309,  0.0030,  0.0105,  ...,  0.0164, -0.0319,  0.0018],\n",
       "        [-0.0260, -0.0251,  0.0266,  ..., -0.0160, -0.0336, -0.0357]],\n",
       "       device='cuda:1')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KorQuAD_Model.state_dict()['qa_outputs.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint2 = torch.load(\"/data/KorQuad_saved_models/model_ep10.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KorQuAD_Model.load_state_dict(checkpoint2['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0310,  0.0029,  0.0100,  ...,  0.0168, -0.0317,  0.0016],\n",
       "        [-0.0259, -0.0253,  0.0265,  ..., -0.0155, -0.0337, -0.0355]],\n",
       "       device='cuda:1')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KorQuAD_Model.state_dict()['qa_outputs.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/byounggeon_kim/anaconda3/envs/test1/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "/home/byounggeon_kim/anaconda3/envs/test1/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/home/byounggeon_kim/anaconda3/envs/test1/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/home/byounggeon_kim/anaconda3/envs/test1/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/byounggeon_kim/anaconda3/envs/test1/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "Caught StopIteration in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/byounggeon_kim/.local/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py\", line 60, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/byounggeon_kim/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/home/byounggeon_kim/2021Project/korean-t5/KQA_Model.py\", line 17, in forward\n    sequence_output, _ = self.bert(input_ids,attention_mask,token_type_ids)\n  File \"/home/byounggeon_kim/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/home/byounggeon_kim/anaconda3/envs/test1/lib/python3.6/site-packages/transformers/modeling_bert.py\", line 734, in forward\n    extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\nStopIteration\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e97e56cb5c00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKorQuAD_Model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKorQuAD_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_positions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m: Caught StopIteration in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/byounggeon_kim/.local/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py\", line 60, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/byounggeon_kim/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/home/byounggeon_kim/2021Project/korean-t5/KQA_Model.py\", line 17, in forward\n    sequence_output, _ = self.bert(input_ids,attention_mask,token_type_ids)\n  File \"/home/byounggeon_kim/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/home/byounggeon_kim/anaconda3/envs/test1/lib/python3.6/site-packages/transformers/modeling_bert.py\", line 734, in forward\n    extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\nStopIteration\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(1000):\n",
    "    meanloss=0\n",
    "    for i,(input_ids, segment_ids, input_mask, start_positions, end_positions) in enumerate(dataloader):\n",
    "        input_ids = torch.tensor(input_ids,dtype=torch.long)\n",
    "        input_ids = input_ids.cuda()\n",
    "        input_mask = torch.tensor(input_mask,dtype=torch.long)\n",
    "        input_mask = input_mask.cuda()\n",
    "        segment_ids = torch.tensor(segment_ids,dtype=torch.long)\n",
    "        segment_ids = segment_ids.cuda()\n",
    "        start_positions = torch.tensor(start_positions,dtype=torch.long)\n",
    "        start_positions = start_positions.cuda()\n",
    "        end_positions = torch.tensor(end_positions,dtype=torch.long)\n",
    "        end_positions = end_positions.cuda()\n",
    "#         KorQuAD_Model = KorQuAD_Model.to(device)\n",
    "    \n",
    "        #optimizer = torch.optim.SGD(mymodel.parameters(), lr=0.0001)\n",
    "        optimizer = torch.optim.SGD(KorQuAD_Model.parameters(), lr=5e-5)\n",
    "        optimizer.zero_grad()\n",
    "        loss = KorQuAD_Model(input_ids, segment_ids, input_mask, start_positions, end_positions)\n",
    "        if (i+1)%100==0:\n",
    "            print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        meanloss+=loss\n",
    "    meanloss=meanloss/i\n",
    "    meanloss=meanloss.item()\n",
    "    if (epoch+1)%10==0:\n",
    "        print(\"epoch:%d  ,   loss:%f\"%((epoch+1),meanloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################TEST\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "bert = BertModel.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=bert(input_ids=test_data[0][0].unsqueeze(0),attention_mask=test_data[0][1].unsqueeze(0),token_type_ids=test_data[0][2].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 768])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   101,  10004,  17196,  46150,  12638,   9745,  25387,   9670,  18622,\n",
       "           9751,  21386,  10530,   8853,  31720,  11102,   9645,  35963,   9773,\n",
       "          11102,   9372,  21928,  10892,    136,    102,   9095,  12030,    113,\n",
       "           4506,   2179,    114,   9632,  59906,  67074,   9372,  21928,  11467,\n",
       "          10250,  47955,   9694, 119145,   9405,  67527,  46150,   9694,   9487,\n",
       "          18623,   9435,  28143,  12030,   8935, 119449,  14279,    117,   9224,\n",
       "          17138, 118888,    117,   9638,  21386,  14523,  36322,   9568,  33188,\n",
       "          13374,   8881,  17138,  12609,    119,  41195,  98068,   9574,  37004,\n",
       "           9281,  42815,  12030,   9638,  65649,    117,   9678, 106589,   9297,\n",
       "          53529,    117,   9638,   9320,  35979,   8857,  79599,   9744,  12945,\n",
       "         109522,  12030,   9425,  31720, 118782,   9297,  12030,  27023,  12092,\n",
       "           8843, 105462,  13374,    117,   9638,  25258,   9568,  33188,  10892,\n",
       "           9953,  25486,  10459,   9434,  45465,  10739,   9714,  17706,    119,\n",
       "           9095,  88236,   9069,  29683,  11261, 105383,  10459,   9751,  46520,\n",
       "          11513,   9651,  60469,  12178,   9954,  12945,    113,   3421,   3169,\n",
       "            114,   9117,  11261,   8908, 103764,   9647,  69708,    117,   9641,\n",
       "          14279,  15387,   9953, 118915,  11882,   9670, 119254,  10622,  64722,\n",
       "          27487,   9519,  25503,  12965,  12424,   9651,  29683,   8996,  10530,\n",
       "           9638,  48599,  14801,   9599,  86488,  47058,   9706, 118767,   9751,\n",
       "          89267,   9568,  33188,   9435,  28143,  11882,   9379, 119082,  12490,\n",
       "            119,   9425,  48418,   9670, 119254,  10739,  41195,   9779,  12030,\n",
       "            113,   4930,   2179,    114,   9559,   9566,  67477,   9102,  12965,\n",
       "           9672,  12092,   8857, 119432,  10622,   9879,  11102,   9365,  20479,\n",
       "          34951,  36553,    113,   3419,   3020,   3378,   4851,    114,   9559,\n",
       "           9779,  41693,  12490,  14867,    117,   9095,  48418,   9670, 119254,\n",
       "          10892,   9460,  12310,    113,   2322,   3591,    114,   9559,   9566,\n",
       "          67477,   9102,  12965,   9779,  13764,    113,   4930,   6457,    114,\n",
       "           9637,   9087, 118782,  14801,   9651,  12310,   9666,  53726,  25605,\n",
       "           9365, 119383,  11513,   9247, 119185,  26737,  11018,   9083,   8900,\n",
       "          86904,   8932,  78123,  34776,    119,   9095,  48418,   8908,  17138,\n",
       "          14279,  22879,   9487,  18623,   9405,  67527,  10459,   8929,  18623,\n",
       "          46150,  11261,   9638,  35866,  65164,  45893,  10004,  17196,  46150,\n",
       "          10459,   9745,  25387,   9670,  18622,  10459,   9751,  21386,  10530,\n",
       "          33378,   8853,  31720,  11102,   9645,  35963,   9773,  12490,    119,\n",
       "            102,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][0].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
